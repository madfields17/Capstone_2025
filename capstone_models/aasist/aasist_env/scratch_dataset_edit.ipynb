{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Standardized_full_data/REAL_train_and_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned file_name column. Example values:\n",
      "0    44784e541ed8d06b757a9dfb02a21ab2\n",
      "1    b1e1dcbc6c82eba5ff40d583220ed8c5\n",
      "2    f87e9ee30d7a221b60bd038686c6ab42\n",
      "3    dfc0f92fe4eb00a4fc817dc2f8ef7170\n",
      "4    5699242804b7856dd2be333128378080\n",
      "Name: file_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove .mp3 and .wav extensions (case-insensitive)\n",
    "df['file_name'] = df['file_name'].str.replace(r'\\.(mp3|wav)$', '', case=False, regex=True)\n",
    "\n",
    "# Optional: save cleaned file\n",
    "df.to_csv('./Standardized_full_data/REAL_train_and_val_cleaned.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Cleaned file_name column. Example values:\")\n",
    "print(df['file_name'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>speaker</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>native_language</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>source</th>\n",
       "      <th>spoof_or_real</th>\n",
       "      <th>train_or_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44784e541ed8d06b757a9dfb02a21ab2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46 - 65</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urdu</td>\n",
       "      <td>Asian, South Asian or Asian American</td>\n",
       "      <td>south asia (india, pakistan, bangladesh, sri l...</td>\n",
       "      <td>asr_fairness</td>\n",
       "      <td>real</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b1e1dcbc6c82eba5ff40d583220ed8c5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31 - 45</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hindi</td>\n",
       "      <td>Asian, South Asian or Asian American</td>\n",
       "      <td>south asia (india, pakistan, bangladesh, sri l...</td>\n",
       "      <td>asr_fairness</td>\n",
       "      <td>real</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f87e9ee30d7a221b60bd038686c6ab42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31 - 45</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hindi</td>\n",
       "      <td>Asian, South Asian or Asian American</td>\n",
       "      <td>south asia (india, pakistan, bangladesh, sri l...</td>\n",
       "      <td>asr_fairness</td>\n",
       "      <td>real</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dfc0f92fe4eb00a4fc817dc2f8ef7170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18 - 22</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>marathi</td>\n",
       "      <td>Asian, South Asian or Asian American</td>\n",
       "      <td>south asia (india, pakistan, bangladesh, sri l...</td>\n",
       "      <td>asr_fairness</td>\n",
       "      <td>real</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5699242804b7856dd2be333128378080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46 - 65</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urdu</td>\n",
       "      <td>Asian, South Asian or Asian American</td>\n",
       "      <td>south asia (india, pakistan, bangladesh, sri l...</td>\n",
       "      <td>asr_fairness</td>\n",
       "      <td>real</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>11214</td>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>south africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>south africa</td>\n",
       "      <td>south africa &amp; southern africa (south africa, ...</td>\n",
       "      <td>in_the_wild</td>\n",
       "      <td>real</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3229</th>\n",
       "      <td>25615</td>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>south africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>south africa</td>\n",
       "      <td>south africa &amp; southern africa (south africa, ...</td>\n",
       "      <td>in_the_wild</td>\n",
       "      <td>real</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>14423</td>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>south africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>south africa</td>\n",
       "      <td>south africa &amp; southern africa (south africa, ...</td>\n",
       "      <td>in_the_wild</td>\n",
       "      <td>real</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3231</th>\n",
       "      <td>7454</td>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>south africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>south africa</td>\n",
       "      <td>south africa &amp; southern africa (south africa, ...</td>\n",
       "      <td>in_the_wild</td>\n",
       "      <td>real</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3232</th>\n",
       "      <td>26278</td>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>south africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>south africa</td>\n",
       "      <td>south africa &amp; southern africa (south africa, ...</td>\n",
       "      <td>in_the_wild</td>\n",
       "      <td>real</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3233 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             file_name         speaker      age  gender  \\\n",
       "0     44784e541ed8d06b757a9dfb02a21ab2             NaN  46 - 65  female   \n",
       "1     b1e1dcbc6c82eba5ff40d583220ed8c5             NaN  31 - 45  female   \n",
       "2     f87e9ee30d7a221b60bd038686c6ab42             NaN  31 - 45  female   \n",
       "3     dfc0f92fe4eb00a4fc817dc2f8ef7170             NaN  18 - 22  female   \n",
       "4     5699242804b7856dd2be333128378080             NaN  46 - 65  female   \n",
       "...                                ...             ...      ...     ...   \n",
       "3228                             11214  Nelson Mandela      NaN    male   \n",
       "3229                             25615  Nelson Mandela      NaN    male   \n",
       "3230                             14423  Nelson Mandela      NaN    male   \n",
       "3231                              7454  Nelson Mandela      NaN    male   \n",
       "3232                             26278  Nelson Mandela      NaN    male   \n",
       "\n",
       "            accent native_language                               country  \\\n",
       "0              NaN            urdu  Asian, South Asian or Asian American   \n",
       "1              NaN           hindi  Asian, South Asian or Asian American   \n",
       "2              NaN           hindi  Asian, South Asian or Asian American   \n",
       "3              NaN         marathi  Asian, South Asian or Asian American   \n",
       "4              NaN            urdu  Asian, South Asian or Asian American   \n",
       "...            ...             ...                                   ...   \n",
       "3228  south africa             NaN                          south africa   \n",
       "3229  south africa             NaN                          south africa   \n",
       "3230  south africa             NaN                          south africa   \n",
       "3231  south africa             NaN                          south africa   \n",
       "3232  south africa             NaN                          south africa   \n",
       "\n",
       "                                                 region        source  \\\n",
       "0     south asia (india, pakistan, bangladesh, sri l...  asr_fairness   \n",
       "1     south asia (india, pakistan, bangladesh, sri l...  asr_fairness   \n",
       "2     south asia (india, pakistan, bangladesh, sri l...  asr_fairness   \n",
       "3     south asia (india, pakistan, bangladesh, sri l...  asr_fairness   \n",
       "4     south asia (india, pakistan, bangladesh, sri l...  asr_fairness   \n",
       "...                                                 ...           ...   \n",
       "3228  south africa & southern africa (south africa, ...   in_the_wild   \n",
       "3229  south africa & southern africa (south africa, ...   in_the_wild   \n",
       "3230  south africa & southern africa (south africa, ...   in_the_wild   \n",
       "3231  south africa & southern africa (south africa, ...   in_the_wild   \n",
       "3232  south africa & southern africa (south africa, ...   in_the_wild   \n",
       "\n",
       "     spoof_or_real train_or_val  \n",
       "0             real        train  \n",
       "1             real        train  \n",
       "2             real        train  \n",
       "3             real        train  \n",
       "4             real        train  \n",
       "...            ...          ...  \n",
       "3228          real          val  \n",
       "3229          real          val  \n",
       "3230          real          val  \n",
       "3231          real          val  \n",
       "3232          real          val  \n",
       "\n",
       "[3233 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = './Standardized_full_data/REAL_train_and_val_cleaned.csv'\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def check_missing_files(real_metadata_path, spoof_metadata_path, base_dir, split=\"train\"):\n",
    "    # Load and standardize both metadata files\n",
    "    real_df = pd.read_csv(real_metadata_path)\n",
    "    spoof_df = pd.read_csv(spoof_metadata_path)\n",
    "\n",
    "    # Print lengths of dataframes\n",
    "    print(f\"Length of real_df: {len(real_df)}\")\n",
    "    print(f\"Length of spoof_df: {len(spoof_df)}\")\n",
    "\n",
    "    # Normalize columns\n",
    "    real_df = real_df.rename(columns={\"file_name\": \"file_name\"})\n",
    "    spoof_df = spoof_df.rename(columns={\"Filename\": \"file_name\"})\n",
    "\n",
    "    # Select relevant columns\n",
    "    real_df = real_df[[\"file_name\", \"spoof_or_real\", \"train_or_val\"]]\n",
    "    spoof_df = spoof_df[[\"file_name\", \"spoof_or_real\", \"train_or_val\"]]\n",
    "\n",
    "    # Add .wav extension\n",
    "    real_df[\"file_name\"] += \".wav\"\n",
    "    spoof_df[\"file_name\"] += \".wav\"\n",
    "\n",
    "    # Combine both metadata sets\n",
    "    combined_df = pd.concat([real_df, spoof_df], ignore_index=True)\n",
    "    print(len(combined_df))\n",
    "    combined_df = combined_df[combined_df[\"train_or_val\"] == split]\n",
    "\n",
    "    # Count total entries\n",
    "    total_metadata = len(combined_df)\n",
    "\n",
    "    # Get existing files from directory\n",
    "    existing_files = set(os.listdir(base_dir))\n",
    "    print(f\"üîç Unique files in base_dir: {len(existing_files)}\")\n",
    "    exists_mask = combined_df[\"file_name\"].isin(existing_files)\n",
    "\n",
    "    num_existing = exists_mask.sum()\n",
    "    num_missing = total_metadata - num_existing\n",
    "    percent_missing = (num_missing / total_metadata) * 100\n",
    "\n",
    "    print(f\"üì¶ Total metadata entries: {total_metadata}\")\n",
    "    print(f\"‚úÖ Existing files in base_dir: {num_existing}\")\n",
    "    print(f\"‚ùå Missing files: {num_missing}\")\n",
    "    print(f\"üìâ Percentage missing: {percent_missing:.2f}%\")\n",
    "\n",
    "    # (Optional) Return missing file names\n",
    "    missing_files = combined_df[~exists_mask][\"file_name\"].tolist()\n",
    "    return missing_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of real_df: 3233\n",
      "Length of spoof_df: 5538\n",
      "8771\n",
      "üîç Unique files in base_dir: 6981\n",
      "üì¶ Total metadata entries: 6656\n",
      "‚úÖ Existing files in base_dir: 6656\n",
      "‚ùå Missing files: 0\n",
      "üìâ Percentage missing: 0.00%\n"
     ]
    }
   ],
   "source": [
    "missing_files = check_missing_files(\n",
    "    real_metadata_path=\"./Standardized_full_data/REAL_train_and_val_cleaned.csv\",\n",
    "    spoof_metadata_path=\"./Standardized_full_data/Metadata TTS data_full_new.csv\",\n",
    "    base_dir=\"./Standardized_full_data/Training\",  # or Val\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of real_df: 3233\n",
      "Length of spoof_df: 5538\n",
      "üîç Unique files in base_dir: 1791\n",
      "üì¶ Total metadata entries: 1791\n",
      "‚úÖ Existing files in base_dir: 1791\n",
      "‚ùå Missing files: 0\n",
      "üìâ Percentage missing: 0.00%\n"
     ]
    }
   ],
   "source": [
    "missing_files = check_missing_files(\n",
    "    real_metadata_path=\"./Standardized_full_data/REAL_train_and_val_cleaned.csv\",\n",
    "    spoof_metadata_path=\"./Standardized_full_data/Metadata TTS data_full_new.csv\",\n",
    "    base_dir=\"./Standardized_full_data/Val\",  # or Val\n",
    "    split=\"val\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_unique_filenames(real_metadata_path, spoof_metadata_path):\n",
    "    # Load CSVs\n",
    "    real_df = pd.read_csv(real_metadata_path)\n",
    "    spoof_df = pd.read_csv(spoof_metadata_path)\n",
    "\n",
    "    # Normalize file name column\n",
    "    real_df = real_df.rename(columns={\"file_name\": \"file_name\"})\n",
    "    spoof_df = spoof_df.rename(columns={\"Filename\": \"file_name\"})\n",
    "\n",
    "    # Add .wav extension\n",
    "    real_df[\"file_name\"] += \".wav\"\n",
    "    spoof_df[\"file_name\"] += \".wav\"\n",
    "\n",
    "    # Get unique filenames\n",
    "    real_files = set(real_df[\"file_name\"].unique())\n",
    "    spoof_files = set(spoof_df[\"file_name\"].unique())\n",
    "\n",
    "    print(f\"üéôÔ∏è Unique REAL files: {len(real_files)}\")\n",
    "    print(f\"ü§ñ Unique SPOOF files: {len(spoof_files)}\")\n",
    "\n",
    "    # Optional: Check overlap\n",
    "    overlap = real_files.intersection(spoof_files)\n",
    "    if overlap:\n",
    "        print(f\"‚ö†Ô∏è Overlapping file names: {len(overlap)}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No overlapping file names between real and spoof.\")\n",
    "\n",
    "    return real_files, spoof_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéôÔ∏è Unique REAL files: 3233\n",
      "ü§ñ Unique SPOOF files: 5537\n",
      "‚úÖ No overlapping file names between real and spoof.\n"
     ]
    }
   ],
   "source": [
    "real_files, spoof_files = check_unique_filenames(\n",
    "    real_metadata_path=\"./Standardized_full_data/REAL_train_and_val_cleaned.csv\",\n",
    "    spoof_metadata_path=\"./Standardized_full_data/Metadata TTS data_full.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def find_unreferenced_wavs_in_split(real_metadata_path, spoof_metadata_path, base_dir, split=\"train\"):\n",
    "    # Load metadata\n",
    "    real_df = pd.read_csv(real_metadata_path)\n",
    "    spoof_df = pd.read_csv(spoof_metadata_path)\n",
    "\n",
    "    # Standardize file name column and add .wav extension\n",
    "    real_df = real_df.rename(columns={\"file_name\": \"file_name\"})\n",
    "    spoof_df = spoof_df.rename(columns={\"Filename\": \"file_name\"})\n",
    "    real_df[\"file_name\"] += \".wav\"\n",
    "    spoof_df[\"file_name\"] += \".wav\"\n",
    "\n",
    "    # Filter by split\n",
    "    real_split = real_df[real_df[\"train_or_val\"] == split]\n",
    "    spoof_split = spoof_df[spoof_df[\"train_or_val\"] == split]\n",
    "\n",
    "    # Combine into one set of metadata file names\n",
    "    referenced_files = set(pd.concat([real_split, spoof_split])[\"file_name\"])\n",
    "\n",
    "    # List all actual .wav files in base_dir\n",
    "    base_files = set(f for f in os.listdir(base_dir) if f.endswith(\".wav\"))\n",
    "\n",
    "    # Find unreferenced files\n",
    "    extra_files = base_files - referenced_files\n",
    "\n",
    "    print(f\"üìÅ Total .wav files in base_dir: {len(base_files)}\")\n",
    "    print(f\"üìÑ Referenced files in metadata for '{split}': {len(referenced_files)}\")\n",
    "    print(f\"‚ùó Unreferenced files in folder (not in metadata): {len(extra_files)}\")\n",
    "\n",
    "    return list(extra_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Total .wav files in base_dir: 6980\n",
      "üìÑ Referenced files in metadata for 'train': 6980\n",
      "‚ùó Unreferenced files in folder (not in metadata): 0\n"
     ]
    }
   ],
   "source": [
    "extra_wavs = find_unreferenced_wavs_in_split(\n",
    "    real_metadata_path=\"./Standardized_full_data/REAL_train_and_val_cleaned.csv\",\n",
    "    spoof_metadata_path=\"./Standardized_full_data/Metadata TTS data_full_new.csv\",\n",
    "    base_dir=\"./Standardized_full_data/Training\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v_output_4964.wav',\n",
       " 'v_output_5015.wav',\n",
       " 'v_output_4889.wav',\n",
       " 'v_output_4930.wav',\n",
       " 'v_output_5090.wav',\n",
       " 'v_output_5011.wav',\n",
       " 'v_output_4842.wav',\n",
       " 'v_output_4968.wav',\n",
       " 'v_output_4883.wav',\n",
       " 'v_output_4974.wav',\n",
       " 'v_output_4954.wav',\n",
       " 'v_output_5062.wav',\n",
       " 'v_output_5029.wav',\n",
       " 'v_output_5105.wav',\n",
       " 'v_output_4845.wav',\n",
       " 'v_output_4919.wav',\n",
       " 'v_output_4898.wav',\n",
       " 'v_output_4923.wav',\n",
       " 'v_output_4993.wav',\n",
       " 'v_output_5087.wav',\n",
       " 'v_output_4942.wav',\n",
       " 'v_output_5034.wav',\n",
       " 'v_output_4951.wav',\n",
       " 'v_output_4882.wav',\n",
       " 'v_output_4859.wav',\n",
       " 'v_output_4885.wav',\n",
       " 'v_output_5097.wav',\n",
       " 'v_output_5123.wav',\n",
       " 'v_output_5124.wav',\n",
       " 'v_output_4907.wav',\n",
       " 'v_output_5093.wav',\n",
       " 'v_output_4933.wav',\n",
       " 'v_output_4847.wav',\n",
       " 'v_output_5131.wav',\n",
       " 'v_output_5054.wav',\n",
       " 'v_output_5024.wav',\n",
       " 'v_output_5028.wav',\n",
       " 'v_output_5113.wav',\n",
       " 'v_output_5042.wav',\n",
       " 'v_output_5030.wav',\n",
       " 'v_output_4826.wav',\n",
       " 'v_output_4928.wav',\n",
       " 'v_output_5031.wav',\n",
       " 'v_output_4912.wav',\n",
       " 'v_output_5035.wav',\n",
       " 'v_output_5066.wav',\n",
       " 'v_output_5116.wav',\n",
       " 'v_output_4952.wav',\n",
       " 'v_output_4881.wav',\n",
       " 'v_output_4888.wav',\n",
       " 'v_output_5056.wav',\n",
       " 'v_output_4858.wav',\n",
       " 'v_output_5069.wav',\n",
       " 'v_output_5038.wav',\n",
       " 'v_output_5070.wav',\n",
       " 'v_output_4819.wav',\n",
       " 'v_output_5119.wav',\n",
       " 'v_output_5137.wav',\n",
       " 'v_output_5101.wav',\n",
       " 'v_output_5032.wav',\n",
       " 'v_output_5046.wav',\n",
       " 'v_output_4861.wav',\n",
       " 'v_output_5065.wav',\n",
       " 'v_output_4977.wav',\n",
       " 'v_output_4996.wav',\n",
       " 'v_output_5027.wav',\n",
       " 'v_output_5052.wav',\n",
       " 'v_output_5134.wav',\n",
       " 'v_output_4934.wav',\n",
       " 'v_output_4944.wav',\n",
       " 'v_output_4925.wav',\n",
       " 'v_output_4906.wav',\n",
       " 'v_output_5141.wav',\n",
       " 'v_output_5077.wav',\n",
       " 'v_output_4879.wav',\n",
       " 'v_output_5103.wav',\n",
       " 'v_output_4932.wav',\n",
       " 'v_output_4991.wav',\n",
       " 'v_output_4911.wav',\n",
       " 'v_output_5080.wav',\n",
       " 'v_output_4976.wav',\n",
       " 'v_output_4891.wav',\n",
       " 'v_output_4821.wav',\n",
       " 'v_output_4914.wav',\n",
       " 'v_output_4856.wav',\n",
       " 'v_output_4983.wav',\n",
       " 'v_output_4887.wav',\n",
       " 'v_output_5022.wav',\n",
       " 'v_output_4999.wav',\n",
       " 'v_output_4946.wav',\n",
       " 'v_output_5096.wav',\n",
       " 'v_output_4832.wav',\n",
       " 'v_output_5001.wav',\n",
       " 'v_output_4841.wav',\n",
       " 'v_output_5089.wav',\n",
       " 'v_output_4880.wav',\n",
       " 'v_output_5058.wav',\n",
       " 'v_output_5025.wav',\n",
       " 'v_output_4997.wav',\n",
       " 'v_output_4920.wav',\n",
       " 'v_output_5110.wav',\n",
       " 'v_output_4965.wav',\n",
       " 'v_output_4905.wav',\n",
       " 'v_output_4918.wav',\n",
       " 'v_output_5072.wav',\n",
       " 'v_output_5039.wav',\n",
       " 'v_output_4903.wav',\n",
       " 'v_output_4926.wav',\n",
       " 'v_output_4904.wav',\n",
       " 'v_output_5121.wav',\n",
       " 'v_output_4922.wav',\n",
       " 'v_output_4987.wav',\n",
       " 'v_output_5036.wav',\n",
       " 'v_output_4970.wav',\n",
       " 'v_output_5142.wav',\n",
       " 'v_output_5057.wav',\n",
       " 'v_output_4875.wav',\n",
       " 'v_output_4902.wav',\n",
       " 'v_output_5108.wav',\n",
       " 'v_output_4901.wav',\n",
       " 'v_output_4876.wav',\n",
       " 'v_output_4935.wav',\n",
       " 'v_output_4927.wav',\n",
       " 'v_output_5050.wav',\n",
       " 'v_output_5005.wav',\n",
       " 'v_output_5099.wav',\n",
       " 'v_output_4834.wav',\n",
       " 'v_output_5064.wav',\n",
       " 'v_output_5043.wav',\n",
       " 'v_output_5139.wav',\n",
       " 'v_output_5114.wav',\n",
       " 'v_output_4966.wav',\n",
       " 'v_output_4900.wav',\n",
       " 'v_output_4824.wav',\n",
       " 'v_output_5120.wav',\n",
       " 'v_output_4866.wav',\n",
       " 'v_output_4950.wav',\n",
       " 'v_output_5104.wav',\n",
       " 'v_output_5002.wav',\n",
       " 'v_output_5003.wav',\n",
       " 'v_output_4963.wav',\n",
       " 'v_output_5136.wav',\n",
       " 'v_output_4979.wav',\n",
       " 'v_output_5061.wav',\n",
       " 'v_output_4969.wav',\n",
       " 'v_output_5081.wav',\n",
       " 'v_output_4857.wav',\n",
       " 'v_output_4820.wav',\n",
       " 'v_output_5086.wav',\n",
       " 'v_output_4981.wav',\n",
       " 'v_output_4980.wav',\n",
       " 'v_output_5135.wav',\n",
       " 'v_output_5044.wav',\n",
       " 'v_output_5075.wav',\n",
       " 'v_output_4992.wav',\n",
       " 'v_output_4986.wav',\n",
       " 'v_output_4955.wav',\n",
       " 'v_output_4862.wav',\n",
       " 'v_output_5033.wav',\n",
       " 'v_output_5129.wav',\n",
       " 'v_output_5051.wav',\n",
       " 'v_output_4830.wav',\n",
       " 'v_output_4985.wav',\n",
       " 'v_output_5107.wav',\n",
       " 'v_output_5063.wav',\n",
       " 'v_output_5014.wav',\n",
       " 'v_output_5021.wav',\n",
       " 'v_output_5115.wav',\n",
       " 'v_output_4939.wav',\n",
       " 'v_output_4860.wav',\n",
       " 'v_output_5053.wav',\n",
       " 'v_output_4864.wav',\n",
       " 'v_output_5118.wav',\n",
       " 'v_output_4998.wav',\n",
       " 'v_output_5013.wav',\n",
       " 'v_output_4948.wav',\n",
       " 'v_output_4838.wav',\n",
       " 'v_output_4886.wav',\n",
       " 'v_output_4899.wav',\n",
       " 'v_output_4958.wav',\n",
       " 'v_output_5076.wav',\n",
       " 'v_output_4943.wav',\n",
       " 'v_output_4994.wav',\n",
       " 'v_output_4973.wav',\n",
       " 'v_output_4872.wav',\n",
       " 'v_output_5130.wav',\n",
       " 'v_output_4962.wav',\n",
       " 'v_output_4825.wav',\n",
       " 'v_output_4967.wav',\n",
       " 'v_output_4961.wav',\n",
       " 'v_output_5111.wav',\n",
       " 'v_output_4897.wav',\n",
       " 'v_output_5126.wav',\n",
       " 'v_output_4877.wav',\n",
       " 'v_output_5092.wav',\n",
       " 'v_output_4936.wav',\n",
       " 'v_output_5009.wav',\n",
       " 'v_output_4941.wav',\n",
       " 'v_output_4975.wav',\n",
       " 'v_output_5112.wav',\n",
       " 'v_output_5138.wav',\n",
       " 'v_output_4917.wav',\n",
       " 'v_output_5055.wav',\n",
       " 'v_output_4829.wav',\n",
       " 'v_output_5122.wav',\n",
       " 'v_output_4978.wav',\n",
       " 'v_output_4827.wav',\n",
       " 'v_output_5106.wav',\n",
       " 'v_output_4870.wav',\n",
       " 'v_output_4871.wav',\n",
       " 'v_output_5102.wav',\n",
       " 'v_output_5040.wav',\n",
       " 'v_output_5127.wav',\n",
       " 'v_output_4868.wav',\n",
       " 'v_output_4910.wav',\n",
       " 'v_output_4822.wav',\n",
       " 'v_output_4895.wav',\n",
       " 'v_output_4844.wav',\n",
       " 'v_output_4956.wav',\n",
       " 'v_output_4940.wav',\n",
       " 'v_output_4937.wav',\n",
       " 'v_output_5133.wav',\n",
       " 'v_output_4896.wav',\n",
       " 'v_output_5037.wav',\n",
       " 'v_output_4971.wav',\n",
       " 'v_output_4850.wav',\n",
       " 'v_output_5132.wav',\n",
       " 'v_output_5067.wav',\n",
       " 'v_output_4846.wav',\n",
       " 'v_output_5019.wav',\n",
       " 'v_output_4893.wav',\n",
       " 'v_output_5074.wav',\n",
       " 'v_output_5109.wav',\n",
       " 'v_output_4921.wav',\n",
       " 'v_output_4839.wav',\n",
       " 'v_output_4836.wav',\n",
       " 'v_output_4924.wav',\n",
       " 'v_output_4828.wav',\n",
       " 'v_output_5084.wav',\n",
       " 'v_output_4855.wav',\n",
       " 'v_output_5017.wav',\n",
       " 'v_output_5091.wav',\n",
       " 'v_output_5117.wav',\n",
       " 'v_output_4869.wav',\n",
       " 'v_output_4884.wav',\n",
       " 'v_output_5071.wav',\n",
       " 'v_output_4835.wav',\n",
       " 'v_output_5016.wav',\n",
       " 'v_output_4831.wav',\n",
       " 'v_output_4863.wav',\n",
       " 'v_output_4878.wav',\n",
       " 'v_output_4843.wav',\n",
       " 'v_output_5083.wav',\n",
       " 'v_output_5012.wav',\n",
       " 'v_output_4909.wav',\n",
       " 'v_output_4938.wav',\n",
       " 'v_output_4995.wav',\n",
       " 'v_output_5128.wav',\n",
       " 'v_output_4908.wav',\n",
       " 'v_output_4867.wav',\n",
       " 'v_output_5140.wav',\n",
       " 'v_output_5018.wav',\n",
       " 'v_output_4874.wav',\n",
       " 'v_output_4988.wav',\n",
       " 'v_output_4972.wav',\n",
       " 'v_output_4947.wav',\n",
       " 'v_output_4959.wav',\n",
       " 'v_output_5006.wav',\n",
       " 'v_output_5088.wav',\n",
       " 'v_output_4957.wav',\n",
       " 'v_output_5098.wav',\n",
       " 'v_output_4892.wav',\n",
       " 'v_output_4913.wav',\n",
       " 'v_output_5073.wav',\n",
       " 'v_output_5026.wav',\n",
       " 'v_output_4953.wav',\n",
       " 'v_output_4989.wav',\n",
       " 'v_output_5047.wav',\n",
       " 'v_output_5079.wav',\n",
       " 'v_output_4945.wav',\n",
       " 'v_output_5004.wav',\n",
       " 'v_output_5082.wav',\n",
       " 'v_output_5094.wav',\n",
       " 'v_output_5068.wav',\n",
       " 'v_output_5125.wav',\n",
       " 'v_output_5023.wav',\n",
       " 'v_output_4865.wav',\n",
       " 'v_output_5041.wav',\n",
       " 'v_output_4848.wav',\n",
       " 'v_output_5059.wav',\n",
       " 'v_output_5010.wav',\n",
       " 'v_output_4960.wav',\n",
       " 'v_output_5007.wav',\n",
       " 'v_output_5078.wav',\n",
       " 'v_output_4890.wav',\n",
       " 'v_output_4929.wav',\n",
       " 'v_output_5060.wav',\n",
       " 'v_output_5095.wav',\n",
       " 'v_output_4823.wav',\n",
       " 'v_output_5020.wav',\n",
       " 'v_output_4949.wav',\n",
       " 'v_output_4916.wav',\n",
       " 'v_output_4840.wav',\n",
       " 'v_output_5000.wav',\n",
       " 'v_output_5008.wav',\n",
       " 'v_output_4990.wav',\n",
       " 'v_output_4931.wav',\n",
       " 'v_output_5100.wav',\n",
       " 'v_output_4854.wav',\n",
       " 'v_output_4852.wav',\n",
       " 'v_output_5085.wav',\n",
       " 'v_output_4915.wav',\n",
       " 'v_output_5045.wav',\n",
       " 'v_output_4837.wav',\n",
       " 'v_output_4849.wav',\n",
       " 'v_output_4873.wav',\n",
       " 'v_output_5048.wav',\n",
       " 'v_output_4982.wav',\n",
       " 'v_output_4833.wav',\n",
       " 'v_output_5049.wav',\n",
       " 'v_output_4984.wav',\n",
       " 'v_output_4894.wav',\n",
       " 'v_output_4853.wav',\n",
       " 'v_output_4851.wav']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exact match found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./Standardized_full_data/Metadata TTS data_full_new.csv\")\n",
    "\n",
    "row = df[df[\"Filename\"] == \"v_output_4964\"]\n",
    "if row.empty:\n",
    "    # See what matches roughly\n",
    "    fuzzy_matches = df[df[\"Filename\"].str.contains(\"v_output_4964\", na=False)]\n",
    "    for f in fuzzy_matches[\"Filename\"]:\n",
    "        print(f\"üß™ Raw: {repr(f)}\")\n",
    "else:\n",
    "    print(\"‚úÖ Exact match found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Is it in base_files? True\n",
      "üß™ Is it in referenced_files? False\n",
      "\n",
      "üéØ Base match candidates:\n",
      "  üìÅ 'v_output_4964.wav'\n",
      "\n",
      "üéØ Referenced match candidates:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Load and clean metadata ---\n",
    "real_df = pd.read_csv(\"./Standardized_full_data/REAL_train_and_val_cleaned.csv\")\n",
    "spoof_df = pd.read_csv(\"./Standardized_full_data/Metadata TTS data_full_new.csv\")\n",
    "\n",
    "# Clean filenames and split column\n",
    "real_df[\"file_name\"] = real_df[\"file_name\"].astype(str).str.strip().str.lower() + \".wav\"\n",
    "spoof_df[\"file_name\"] = spoof_df[\"Filename\"].astype(str).str.strip().str.lower() + \".wav\"\n",
    "real_df[\"train_or_val\"] = real_df[\"train_or_val\"].astype(str).str.strip().str.lower()\n",
    "spoof_df[\"train_or_val\"] = spoof_df[\"train_or_val\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Filter for train split\n",
    "split = \"train\"\n",
    "real_split = real_df[real_df[\"train_or_val\"] == split]\n",
    "spoof_split = spoof_df[spoof_df[\"train_or_val\"] == split]\n",
    "\n",
    "# Referenced files from metadata\n",
    "referenced_files = set(pd.concat([real_split, spoof_split])[\"file_name\"])\n",
    "\n",
    "# Files in base directory\n",
    "base_dir = \"./Standardized_full_data/Training\"\n",
    "base_files = set(f.lower().strip() for f in os.listdir(base_dir) if f.endswith(\".wav\"))\n",
    "\n",
    "# Now check your file!\n",
    "target = \"v_output_4964.wav\"\n",
    "\n",
    "print(\"üß™ Is it in base_files?\", target in base_files)\n",
    "print(\"üß™ Is it in referenced_files?\", target in referenced_files)\n",
    "\n",
    "print(\"\\nüéØ Base match candidates:\")\n",
    "for f in base_files:\n",
    "    if \"v_output_4964\" in f:\n",
    "        print(\"  üìÅ\", repr(f))\n",
    "\n",
    "print(\"\\nüéØ Referenced match candidates:\")\n",
    "for f in referenced_files:\n",
    "    if \"v_output_4964\" in f:\n",
    "        print(\"  üìÑ\", repr(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>speaker</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>native_language</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>source</th>\n",
       "      <th>spoof_or_real</th>\n",
       "      <th>train_or_val</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Accent</th>\n",
       "      <th>Eleven_Labs_voice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4958</th>\n",
       "      <td>v_output_4964.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>south asia (india, pakistan, bangladesh, sri l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spoof</td>\n",
       "      <td>nan</td>\n",
       "      <td>v_output_4964</td>\n",
       "      <td>The old, weathered book on the dusty shelf yea...</td>\n",
       "      <td>female</td>\n",
       "      <td>Hindi Indian</td>\n",
       "      <td>Diana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              file_name speaker  age gender accent native_language country  \\\n",
       "4958  v_output_4964.wav     NaN  NaN    NaN    NaN             NaN     NaN   \n",
       "\n",
       "                                                 region source spoof_or_real  \\\n",
       "4958  south asia (india, pakistan, bangladesh, sri l...    NaN         spoof   \n",
       "\n",
       "     train_or_val       Filename  \\\n",
       "4958          nan  v_output_4964   \n",
       "\n",
       "                                               Sentence  Gender        Accent  \\\n",
       "4958  The old, weathered book on the dusty shelf yea...  female  Hindi Indian   \n",
       "\n",
       "     Eleven_Labs_voice  \n",
       "4958             Diana  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_df = pd.concat([real_df, spoof_df])\n",
    "matches = all_df[all_df[\"file_name\"].str.contains(\"v_output_4964\")]\n",
    "display(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Accent</th>\n",
       "      <th>Eleven_Labs_voice</th>\n",
       "      <th>train_or_val</th>\n",
       "      <th>region</th>\n",
       "      <th>spoof_or_real</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4958</th>\n",
       "      <td>v_output_4964</td>\n",
       "      <td>The old, weathered book on the dusty shelf yea...</td>\n",
       "      <td>female</td>\n",
       "      <td>Hindi Indian</td>\n",
       "      <td>Diana</td>\n",
       "      <td>nan</td>\n",
       "      <td>south asia (india, pakistan, bangladesh, sri l...</td>\n",
       "      <td>spoof</td>\n",
       "      <td>v_output_4964.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Filename                                           Sentence  \\\n",
       "4958  v_output_4964  The old, weathered book on the dusty shelf yea...   \n",
       "\n",
       "      Gender        Accent Eleven_Labs_voice train_or_val  \\\n",
       "4958  female  Hindi Indian             Diana          nan   \n",
       "\n",
       "                                                 region spoof_or_real  \\\n",
       "4958  south asia (india, pakistan, bangladesh, sri l...         spoof   \n",
       "\n",
       "              file_name  \n",
       "4958  v_output_4964.wav  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matches = spoof_df[spoof_df[\"file_name\"].str.contains(\"v_output_4964\")]\n",
    "display(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Missing 'train_or_val' in REAL metadata: 0\n",
      "üß™ Missing 'train_or_val' in SPOOF metadata: 324\n",
      "üßÆ Total missing entries: 324\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load metadata\n",
    "real_df = pd.read_csv(\"./Standardized_full_data/REAL_train_and_val_cleaned.csv\")\n",
    "spoof_df = pd.read_csv(\"./Standardized_full_data/Metadata TTS data_full_new.csv\")\n",
    "\n",
    "# Check for NaNs in train_or_val column\n",
    "real_missing = real_df[\"train_or_val\"].isna().sum()\n",
    "spoof_missing = spoof_df[\"train_or_val\"].isna().sum()\n",
    "\n",
    "print(f\"üß™ Missing 'train_or_val' in REAL metadata: {real_missing}\")\n",
    "print(f\"üß™ Missing 'train_or_val' in SPOOF metadata: {spoof_missing}\")\n",
    "print(f\"üßÆ Total missing entries: {real_missing + spoof_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aasist-env-_0UawaMH-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
